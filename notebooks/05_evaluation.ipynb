{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133c169f-5bbc-4881-9ba6-2a1302e8894c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned embeddings: (43916, 2048)\n",
      "Baseline embeddings:   (43916, 2048)\n",
      "Test set queries:      6588\n",
      "FAISS indexes built:   ✅\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 1: Load Everything for Evaluation\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import time\n",
    "\n",
    "# Paths\n",
    "EMBED_DIR = '../data/embeddings'\n",
    "DATA_DIR = '../data/processed'\n",
    "\n",
    "# Load embeddings\n",
    "ft_embeddings = np.load(f'{EMBED_DIR}/finetuned_embeddings.npy').astype(np.float32)\n",
    "ft_ids = np.load(f'{EMBED_DIR}/finetuned_ids.npy')\n",
    "bl_embeddings = np.load(f'{EMBED_DIR}/baseline_embeddings.npy').astype(np.float32)\n",
    "bl_ids = np.load(f'{EMBED_DIR}/baseline_ids.npy')\n",
    "\n",
    "# Normalize\n",
    "faiss.normalize_L2(ft_embeddings)\n",
    "faiss.normalize_L2(bl_embeddings)\n",
    "\n",
    "# Load metadata\n",
    "df = pd.read_csv(f'{DATA_DIR}/filtered_styles.csv')\n",
    "test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n",
    "\n",
    "# Build FAISS indexes for both\n",
    "ft_index = faiss.IndexFlatL2(ft_embeddings.shape[1])\n",
    "ft_index.add(ft_embeddings)\n",
    "\n",
    "bl_index = faiss.IndexFlatL2(bl_embeddings.shape[1])\n",
    "bl_index.add(bl_embeddings)\n",
    "\n",
    "# Create lookup: image_id → article type\n",
    "id_to_type = dict(zip(df['id'], df['articleType']))\n",
    "\n",
    "print(f\"Fine-tuned embeddings: {ft_embeddings.shape}\")\n",
    "print(f\"Baseline embeddings:   {bl_embeddings.shape}\")\n",
    "print(f\"Test set queries:      {len(test_df)}\")\n",
    "print(f\"FAISS indexes built:   ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e71b4b58-41c8-4ee3-81f3-2daae6ff4215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined ✅\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 2: Define Evaluation Metrics\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_search(index, embeddings, ids, query_ids, id_to_type, k=10):\n",
    "    \"\"\"\n",
    "    Run evaluation on a set of queries.\n",
    "    \n",
    "    Parameters:\n",
    "        index:      FAISS index to search\n",
    "        embeddings: all embeddings (to find query vector by position)\n",
    "        ids:        all image IDs (to convert position → image ID)\n",
    "        query_ids:  list of image IDs to use as queries\n",
    "        id_to_type: dict mapping image_id → articleType\n",
    "        k:          how many results to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        dictionary with all metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Storage for all metrics\n",
    "    precisions = {1: [], 5: [], 10: []}   # P@1, P@5, P@10\n",
    "    recalls = {1: [], 5: [], 10: []}       # R@1, R@5, R@10\n",
    "    reciprocal_ranks = []                   # for MRR\n",
    "    average_precisions = []                 # for mAP\n",
    "    \n",
    "    # We need to know: for each query, how many TOTAL correct items exist?\n",
    "    # = how many images share the same articleType in the ENTIRE index\n",
    "    \n",
    "    for query_id in query_ids:\n",
    "        \n",
    "        # --- STEP 1: Find query's position and type ---\n",
    "        query_positions = np.where(ids == query_id)[0]\n",
    "        if len(query_positions) == 0:\n",
    "            continue    # skip if query not found in embeddings\n",
    "        \n",
    "        query_pos = query_positions[0]\n",
    "        query_type = id_to_type.get(query_id)\n",
    "        if query_type is None:\n",
    "            continue    # skip if no metadata\n",
    "        \n",
    "        # --- STEP 2: Search FAISS ---\n",
    "        query_vector = embeddings[query_pos].reshape(1, -1)\n",
    "        distances, indices = index.search(query_vector, k + 1)  # k+1 because first result is the query itself\n",
    "        \n",
    "        # Remove the query itself from results (first result, distance=0)\n",
    "        result_positions = indices[0][1:]   # skip first result\n",
    "        result_ids = [ids[pos] for pos in result_positions]\n",
    "        \n",
    "        # --- STEP 3: Check each result — correct or wrong? ---\n",
    "        correct = []    # list of True/False for each result\n",
    "        for rid in result_ids:\n",
    "            result_type = id_to_type.get(rid)\n",
    "            correct.append(result_type == query_type)\n",
    "        \n",
    "        # correct = [True, False, True, True, False, True, False, ...]\n",
    "        #            rank1  rank2  rank3  rank4  rank5  rank6  rank7\n",
    "        \n",
    "        # --- STEP 4: Precision@K ---\n",
    "        for k_val in [1, 5, 10]:\n",
    "            if k_val <= len(correct):\n",
    "                num_correct = sum(correct[:k_val])     # count True in top k_val\n",
    "                precisions[k_val].append(num_correct / k_val)\n",
    "        \n",
    "        # --- STEP 5: Recall@K ---\n",
    "        # Total number of same-type images in dataset (minus the query itself)\n",
    "        total_relevant = sum(1 for iid in ids if id_to_type.get(iid) == query_type) - 1\n",
    "        \n",
    "        for k_val in [1, 5, 10]:\n",
    "            if k_val <= len(correct):\n",
    "                num_correct = sum(correct[:k_val])\n",
    "                recalls[k_val].append(num_correct / total_relevant)\n",
    "        \n",
    "        # --- STEP 6: Reciprocal Rank (for MRR) ---\n",
    "        rr = 0.0\n",
    "        for rank, is_correct in enumerate(correct):\n",
    "            if is_correct:\n",
    "                rr = 1.0 / (rank + 1)    # rank is 0-indexed, so +1\n",
    "                break\n",
    "        reciprocal_ranks.append(rr)\n",
    "        \n",
    "        # --- STEP 7: Average Precision (for mAP) ---\n",
    "        ap = 0.0\n",
    "        num_correct_so_far = 0\n",
    "        for rank, is_correct in enumerate(correct):\n",
    "            if is_correct:\n",
    "                num_correct_so_far += 1\n",
    "                precision_at_rank = num_correct_so_far / (rank + 1)\n",
    "                ap += precision_at_rank\n",
    "        \n",
    "        # Divide by total correct found (not total relevant)\n",
    "        num_correct_total = sum(correct)\n",
    "        if num_correct_total > 0:\n",
    "            ap = ap / num_correct_total\n",
    "        average_precisions.append(ap)\n",
    "    \n",
    "    # --- STEP 8: Average all metrics ---\n",
    "    results = {\n",
    "        'P@1':  np.mean(precisions[1]),\n",
    "        'P@5':  np.mean(precisions[5]),\n",
    "        'P@10': np.mean(precisions[10]),\n",
    "        'R@1':  np.mean(recalls[1]),\n",
    "        'R@5':  np.mean(recalls[5]),\n",
    "        'R@10': np.mean(recalls[10]),\n",
    "        'MRR':  np.mean(reciprocal_ranks),\n",
    "        'mAP@10': np.mean(average_precisions),\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation function defined ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f4cab01-b3bc-4437-bd13-2df8e903ecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on 6588 test queries...\n",
      "\n",
      "Evaluating fine-tuned model...\n",
      "  Done in 186.6s\n",
      "\n",
      "Evaluating baseline model...\n",
      "  Done in 197.8s\n",
      "\n",
      "=================================================================\n",
      "EVALUATION RESULTS: Baseline vs Fine-tuned\n",
      "Test set: 6588 queries\n",
      "=================================================================\n",
      "Metric           Baseline   Fine-tuned       Change\n",
      "-----------------------------------------------------------------\n",
      "P@1               83.26%      86.23%    ▲  +2.98%\n",
      "P@5               78.04%      82.61%    ▲  +4.57%\n",
      "P@10              75.46%      81.12%    ▲  +5.66%\n",
      "R@1                0.15%       0.16%    ─  +0.00%\n",
      "R@5                0.64%       0.72%    ─  +0.08%\n",
      "R@10               1.18%       1.39%    ▲  +0.21%\n",
      "MRR               88.28%      90.22%    ▲  +1.94%\n",
      "mAP@10            83.60%      86.79%    ▲  +3.19%\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 3: Run Evaluation — Baseline vs Fine-tuned\n",
    "# ============================================================\n",
    "import time\n",
    "\n",
    "# Get test set image IDs\n",
    "test_ids = test_df['id'].values\n",
    "print(f\"Evaluating on {len(test_ids)} test queries...\\n\")\n",
    "\n",
    "# --- Evaluate Fine-tuned Model ---\n",
    "print(\"Evaluating fine-tuned model...\")\n",
    "start = time.time()\n",
    "ft_results = evaluate_search(ft_index, ft_embeddings, ft_ids, test_ids, id_to_type, k=10)\n",
    "ft_time = time.time() - start\n",
    "print(f\"  Done in {ft_time:.1f}s\\n\")\n",
    "\n",
    "# --- Evaluate Baseline Model ---\n",
    "print(\"Evaluating baseline model...\")\n",
    "start = time.time()\n",
    "bl_results = evaluate_search(bl_index, bl_embeddings, bl_ids, test_ids, id_to_type, k=10)\n",
    "bl_time = time.time() - start\n",
    "print(f\"  Done in {bl_time:.1f}s\\n\")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"=\" * 65)\n",
    "print(\"EVALUATION RESULTS: Baseline vs Fine-tuned\")\n",
    "print(f\"Test set: {len(test_ids)} queries\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Metric':<12} {'Baseline':>12} {'Fine-tuned':>12} {'Change':>12}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for metric in ['P@1', 'P@5', 'P@10', 'R@1', 'R@5', 'R@10', 'MRR', 'mAP@10']:\n",
    "    bl_val = bl_results[metric]\n",
    "    ft_val = ft_results[metric]\n",
    "    diff = ft_val - bl_val\n",
    "    arrow = \"▲\" if diff > 0.001 else (\"▼\" if diff < -0.001 else \"─\")\n",
    "    print(f\"{metric:<12} {bl_val:>11.2%} {ft_val:>11.2%} {arrow:>4} {diff:>+7.2%}\")\n",
    "\n",
    "print(\"=\" * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8813b0f8-a54b-4d6c-99f4-39c4caeb61fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PER-CATEGORY RESULTS (Top 20 categories, Fine-tuned model)\n",
      "================================================================================\n",
      "Category                Count      P@5     P@10      MRR   mAP@10\n",
      "--------------------------------------------------------------------------------\n",
      "Tshirts                  1060   90.3%   89.4%   95.1%   92.8%\n",
      "Shirts                    483   94.1%   93.6%   97.2%   95.6%\n",
      "Casual Shoes              427   73.5%   72.4%   86.5%   80.0%\n",
      "Watches                   382   99.2%   98.8%  100.0%   99.6%\n",
      "Sports Shoes              306   75.7%   74.1%   86.7%   80.8%\n",
      "Kurtas                    277   78.4%   77.3%   87.1%   82.7%\n",
      "Tops                      265   54.3%   51.5%   78.9%   67.4%\n",
      "Handbags                  264   93.6%   91.8%   97.6%   95.2%\n",
      "Heels                     199   75.7%   73.7%   89.5%   82.2%\n",
      "Sunglasses                161  100.0%  100.0%  100.0%  100.0%\n",
      "Wallets                   141   90.9%   89.9%   95.5%   92.8%\n",
      "Flip Flops                137   84.2%   82.6%   91.3%   87.2%\n",
      "Sandals                   135   73.6%   72.2%   86.5%   81.7%\n",
      "Briefs                    128   98.4%   97.9%   99.3%   99.0%\n",
      "Belts                     122   99.0%   98.9%   98.8%   98.9%\n",
      "Backpacks                 109   90.1%   89.1%   96.4%   94.2%\n",
      "Socks                     103   96.3%   95.1%   98.4%   97.4%\n",
      "Formal Shoes               96   81.2%   80.9%   87.7%   85.2%\n",
      "Perfume and Body Mist      92   85.7%   82.9%   88.9%   87.9%\n",
      "Jeans                      91   87.5%   87.9%   92.7%   90.6%\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Best category:  Sunglasses (mAP@10 = 100.0%)\n",
      "Worst category: Tops (mAP@10 = 67.4%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 4: Per-Category Evaluation\n",
    "# ============================================================\n",
    "\n",
    "# Get top 20 categories by count in test set\n",
    "top_categories = test_df['articleType'].value_counts().head(20).index.tolist()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PER-CATEGORY RESULTS (Top 20 categories, Fine-tuned model)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Category':<22} {'Count':>6} {'P@5':>8} {'P@10':>8} {'MRR':>8} {'mAP@10':>8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "category_results = {}\n",
    "\n",
    "for category in top_categories:\n",
    "    # Get test images for this category\n",
    "    cat_test_ids = test_df[test_df['articleType'] == category]['id'].values\n",
    "    \n",
    "    # Evaluate\n",
    "    cat_results = evaluate_search(\n",
    "        ft_index, ft_embeddings, ft_ids, cat_test_ids, id_to_type, k=10\n",
    "    )\n",
    "    category_results[category] = cat_results\n",
    "    \n",
    "    print(f\"{category:<22} {len(cat_test_ids):>6} {cat_results['P@5']:>7.1%} \"\n",
    "          f\"{cat_results['P@10']:>7.1%} {cat_results['MRR']:>7.1%} {cat_results['mAP@10']:>7.1%}\")\n",
    "\n",
    "# Find best and worst\n",
    "print(\"-\" * 80)\n",
    "best_cat = max(category_results.items(), key=lambda x: x[1]['mAP@10'])\n",
    "worst_cat = min(category_results.items(), key=lambda x: x[1]['mAP@10'])\n",
    "\n",
    "print(f\"\\nBest category:  {best_cat[0]} (mAP@10 = {best_cat[1]['mAP@10']:.1%})\")\n",
    "print(f\"Worst category: {worst_cat[0]} (mAP@10 = {worst_cat[1]['mAP@10']:.1%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "615dfc5c-d87a-40cb-b701-8e199c5847b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "LATENCY BENCHMARK\n",
      "=======================================================\n",
      "\n",
      "Single query (Flat Index, k=10):\n",
      "  Average:  7.89 ms\n",
      "  Median:   6.81 ms\n",
      "  P95:      7.52 ms\n",
      "  P99:      9.16 ms\n",
      "\n",
      "Batch size   1:     6.88 ms total | 6.88 ms/query | 145 queries/sec\n",
      "\n",
      "Batch size  10:    33.45 ms total | 3.34 ms/query | 299 queries/sec\n",
      "\n",
      "Batch size  50:    37.48 ms total | 0.75 ms/query | 1334 queries/sec\n",
      "\n",
      "Batch size 100:    46.16 ms total | 0.46 ms/query | 2166 queries/sec\n",
      "\n",
      "=======================================================\n",
      "PRODUCTION READINESS:\n",
      "  Target: < 100ms per query for real-time search\n",
      "  Actual: 0.46 ms per query\n",
      "  Status: ✅ READY\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CELL 5: Latency Benchmark\n",
    "# ============================================================\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"LATENCY BENCHMARK\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Single query latency (average over 100 runs)\n",
    "query_vector = ft_embeddings[0].reshape(1, -1)\n",
    "\n",
    "times = []\n",
    "for _ in range(100):\n",
    "    start = time.time()\n",
    "    distances, indices = ft_index.search(query_vector, k=10)\n",
    "    elapsed = time.time() - start\n",
    "    times.append(elapsed * 1000)  # convert to ms\n",
    "\n",
    "print(f\"\\nSingle query (Flat Index, k=10):\")\n",
    "print(f\"  Average:  {np.mean(times):.2f} ms\")\n",
    "print(f\"  Median:   {np.median(times):.2f} ms\")\n",
    "print(f\"  P95:      {np.percentile(times, 95):.2f} ms\")\n",
    "print(f\"  P99:      {np.percentile(times, 99):.2f} ms\")\n",
    "\n",
    "# Batch query latency\n",
    "for batch_size in [1, 10, 50, 100]:\n",
    "    query_vectors = ft_embeddings[:batch_size]\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(20):\n",
    "        start = time.time()\n",
    "        ft_index.search(query_vectors, k=10)\n",
    "        elapsed = time.time() - start\n",
    "        times.append(elapsed * 1000)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    per_query = avg_time / batch_size\n",
    "    qps = batch_size / (avg_time / 1000)\n",
    "    print(f\"\\nBatch size {batch_size:>3}: {avg_time:>8.2f} ms total | \"\n",
    "          f\"{per_query:.2f} ms/query | {qps:.0f} queries/sec\")\n",
    "\n",
    "print(f\"\\n{'=' * 55}\")\n",
    "print(\"PRODUCTION READINESS:\")\n",
    "print(f\"  Target: < 100ms per query for real-time search\")\n",
    "print(f\"  Actual: {np.mean(times)/100:.2f} ms per query\")\n",
    "print(f\"  Status: {'✅ READY' if np.mean(times)/100 < 100 else '❌ TOO SLOW'}\")\n",
    "print(f\"{'=' * 55}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e54f7-c7b1-4d83-927e-9c2bfe1fa6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
